#!/usr/bin/env python3

import torch
import numpy as np
import sys
import os

# åˆ‡æ¢åˆ°æ­£ç¡®çš„å·¥ä½œç›®å½•
os.chdir('/home/lin/youkechuiguo/youkechuiguo_robot/unitree_IL_lerobot')
sys.path.insert(0, '/home/lin/youkechuiguo/youkechuiguo_robot/unitree_IL_lerobot')

from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from lerobot.common.policies.factory import make_policy
import matplotlib.pyplot as plt

def analyze_multimodal_data():
    """åˆ†æå¤šæ¨¡æ€æ•°æ®å¤„ç†æµç¨‹ï¼Œé‡ç‚¹æ£€æŸ¥è§†é¢‘å’ŒçŠ¶æ€æ•°æ®"""
    
    print("ğŸ” åˆ†æå¤šæ¨¡æ€æ•°æ®å¤„ç†æµç¨‹")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = LeRobotDataset("/home/lin/youkechuiguo/dataset/G1_ObjectPlacement_Dataset")
    
    print(f"æ•°æ®é›†æ€»é•¿åº¦: {len(dataset)}")
    print(f"æ•°æ®é›†ç‰¹å¾: {list(dataset.features.keys())}")
    
    # åˆ†æä¸åŒæ ·æœ¬çš„æ•°æ®
    print("\nğŸ” åˆ†æä¸åŒæ ·æœ¬çš„å¤šæ¨¡æ€æ•°æ®")
    print("-" * 60)
    
    # é€‰æ‹©æ¥è‡ªä¸åŒepisodeçš„æ ·æœ¬
    test_indices = [0, 320, 642, 960]  # ä¸åŒepisodeçš„èµ·å§‹ç‚¹
    
    for i, idx in enumerate(test_indices):
        if idx >= len(dataset):
            continue
            
        print(f"\næ ·æœ¬ {idx} (Episode {idx//320}):")
        sample = dataset[idx]
        
        # åˆ†æçŠ¶æ€æ•°æ®
        if "observation.state" in sample:
            state = sample["observation.state"].numpy()
            print(f"  çŠ¶æ€æ•°æ®: shape={state.shape}, mean={state.mean():.4f}, std={state.std():.4f}")
            print(f"  çŠ¶æ€èŒƒå›´: [{state.min():.4f}, {state.max():.4f}]")
            print(f"  å‰5ç»´: {state[:5]}")
        
        # åˆ†æè§†é¢‘æ•°æ®
        video_keys = [k for k in sample.keys() if k.startswith("observation.images")]
        print(f"  è§†é¢‘æ•°æ®: {len(video_keys)} ä¸ªæ‘„åƒå¤´")
        
        for video_key in video_keys:
            video = sample[video_key]
            print(f"    {video_key}: shape={video.shape}, dtype={video.dtype}")
            print(f"      åƒç´ èŒƒå›´: [{video.min():.3f}, {video.max():.3f}]")
            print(f"      åƒç´ å‡å€¼: {video.mean():.3f}")
    
    # æ£€æŸ¥æ•°æ®å˜åŒ–æ€§
    print(f"\nğŸ” æ£€æŸ¥æ•°æ®å˜åŒ–æ€§")
    print("-" * 60)
    
    states = []
    videos = {key: [] for key in ["observation.images.cam_left_high", 
                                  "observation.images.cam_right_high",
                                  "observation.images.cam_left_wrist", 
                                  "observation.images.cam_right_wrist"]}
    
    # æ”¶é›†å¤šä¸ªæ ·æœ¬çš„æ•°æ®
    for idx in [0, 100, 320, 420, 642, 742]:
        if idx >= len(dataset):
            continue
        sample = dataset[idx]
        
        if "observation.state" in sample:
            states.append(sample["observation.state"].numpy())
        
        for video_key in videos.keys():
            if video_key in sample:
                # è®¡ç®—è§†é¢‘å¸§çš„å‡å€¼ä½œä¸ºç‰¹å¾
                video_mean = sample[video_key].float().mean().item()
                videos[video_key].append(video_mean)
    
    # åˆ†æçŠ¶æ€æ•°æ®å˜åŒ–æ€§
    if states:
        states = np.array(states)
        print(f"çŠ¶æ€æ•°æ®å˜åŒ–æ€§:")
        print(f"  æ ·æœ¬é—´å‡å€¼å·®å¼‚: {states.std(axis=0).mean():.6f}")
        print(f"  æœ€å¤§å˜åŒ–ç»´åº¦: {states.std(axis=0).max():.6f}")
        print(f"  æœ€å°å˜åŒ–ç»´åº¦: {states.std(axis=0).min():.6f}")
    
    # åˆ†æè§†é¢‘æ•°æ®å˜åŒ–æ€§
    print(f"è§†é¢‘æ•°æ®å˜åŒ–æ€§:")
    for video_key, values in videos.items():
        if values:
            values = np.array(values)
            print(f"  {video_key}: std={values.std():.6f}, range=[{values.min():.3f}, {values.max():.3f}]")
    
    # åŠ è½½æ¨¡å‹å¹¶æµ‹è¯•è¾“å…¥å¤„ç†
    print(f"\nğŸ” æµ‹è¯•æ¨¡å‹è¾“å…¥å¤„ç†")
    print("-" * 60)
    
    policy = make_policy(
        hydra_cfg_path="/home/lin/youkechuiguo/youkechuiguo_robot/config/eval/eval_g1_dataset.yaml",
        pretrained_policy_name_or_path="/home/lin/youkechuiguo/model/smolvla_trained/last",
        dataset_repo_id="/home/lin/youkechuiguo/dataset/G1_ObjectPlacement_Dataset"
    )
    policy.eval()
    
    # æµ‹è¯•ä¸åŒæ ·æœ¬çš„æ¨¡å‹è¾“å…¥
    print("æµ‹è¯•æ¨¡å‹å¯¹ä¸åŒè¾“å…¥çš„å“åº”:")
    
    predictions = []
    input_features = []
    
    for i, idx in enumerate([0, 320, 642]):
        if idx >= len(dataset):
            continue
            
        sample = dataset[idx]
        
        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        batch = {}
        for key, value in sample.items():
            if key.startswith("observation"):
                batch[key] = value.unsqueeze(0).cuda()
        
        batch["task"] = "object placement"
        if hasattr(policy, 'language_tokenizer'):
            batch["language_instruction"] = ["place the object"]
        
        # è·å–é¢„æµ‹
        with torch.no_grad():
            pred = policy.select_action(batch).cpu().numpy().flatten()
            predictions.append(pred)
        
        # æå–è¾“å…¥ç‰¹å¾ç”¨äºåˆ†æ
        features = []
        if "observation.state" in sample:
            features.extend(sample["observation.state"].numpy()[:5])  # å‰5ç»´çŠ¶æ€
        
        # æ·»åŠ è§†é¢‘ç‰¹å¾ï¼ˆæ¯ä¸ªæ‘„åƒå¤´çš„å‡å€¼ï¼‰
        for video_key in ["observation.images.cam_left_high", "observation.images.cam_right_high"]:
            if video_key in sample:
                features.append(sample[video_key].float().mean().item())
        
        input_features.append(features)
        
        print(f"  æ ·æœ¬ {idx}: pred[0:3]={pred[:3]:.4f}")
    
    predictions = np.array(predictions)
    input_features = np.array(input_features)
    
    print(f"\nğŸ“Š è¾“å…¥-è¾“å‡ºåˆ†æ:")
    print(f"è¾“å…¥ç‰¹å¾å˜åŒ–æ€§: {input_features.std(axis=0).mean():.6f}")
    print(f"è¾“å‡ºé¢„æµ‹å˜åŒ–æ€§: {predictions.std(axis=0).mean():.6f}")
    
    # æ£€æŸ¥è¾“å…¥ç‰¹å¾ä¸è¾“å‡ºçš„ç›¸å…³æ€§
    if len(input_features) > 1 and len(predictions) > 1:
        input_var = input_features.std(axis=0)
        output_var = predictions.std(axis=0)
        
        print(f"è¾“å…¥å˜åŒ–æ€§èŒƒå›´: [{input_var.min():.6f}, {input_var.max():.6f}]")
        print(f"è¾“å‡ºå˜åŒ–æ€§èŒƒå›´: [{output_var.min():.6f}, {output_var.max():.6f}]")
    
    # æµ‹è¯•æ¨¡å‹å†…éƒ¨å¤„ç†
    print(f"\nğŸ” æµ‹è¯•æ¨¡å‹å†…éƒ¨å¤„ç†")
    print("-" * 60)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®å¤„ç†è§†é¢‘è¾“å…¥
    sample = dataset[642]
    batch = {}
    for key, value in sample.items():
        if key.startswith("observation"):
            batch[key] = value.unsqueeze(0).cuda()
    
    batch["task"] = "object placement"
    if hasattr(policy, 'language_tokenizer'):
        batch["language_instruction"] = ["place the object"]
    
    # æ£€æŸ¥æ¨¡å‹å†…éƒ¨çš„normalize_inputs
    print("æ£€æŸ¥è¾“å…¥æ ‡å‡†åŒ–:")
    if hasattr(policy, 'normalize_inputs'):
        normalized_batch = policy.normalize_inputs(batch)
        
        for key in batch.keys():
            if key.startswith("observation"):
                original = batch[key]
                normalized = normalized_batch[key]
                
                if original.dtype == torch.float32:  # åªæ£€æŸ¥æ•°å€¼æ•°æ®
                    print(f"  {key}:")
                    print(f"    åŸå§‹: mean={original.mean():.4f}, std={original.std():.4f}")
                    print(f"    æ ‡å‡†åŒ–: mean={normalized.mean():.4f}, std={normalized.std():.4f}")
    
    # æœ€ç»ˆè¯Šæ–­
    print(f"\n" + "=" * 80)
    print(f"ğŸ¥ å¤šæ¨¡æ€æ•°æ®è¯Šæ–­")
    print(f"=" * 80)
    
    input_variation = input_features.std(axis=0).mean() if len(input_features) > 1 else 0
    output_variation = predictions.std(axis=0).mean() if len(predictions) > 1 else 0
    
    if input_variation > 0.01 and output_variation < 0.01:
        print(f"âŒ å‘ç°é—®é¢˜: è¾“å…¥æ•°æ®æœ‰å˜åŒ– ({input_variation:.6f}) ä½†è¾“å‡ºå‡ ä¹ä¸å˜ ({output_variation:.6f})")
        print(f"   å¯èƒ½åŸå› :")
        print(f"   1. æ¨¡å‹æ²¡æœ‰å­¦ä¼šå¤„ç†è§†è§‰è¾“å…¥")
        print(f"   2. è§†è§‰ç¼–ç å™¨æƒé‡æœªæ­£ç¡®åŠ è½½")
        print(f"   3. è¾“å…¥é¢„å¤„ç†æœ‰é—®é¢˜")
        print(f"   4. æ¨¡å‹è¿‡åº¦ä¾èµ–æŸç§è¾“å…¥æ¨¡æ€")
    elif input_variation < 0.01:
        print(f"âŒ å‘ç°é—®é¢˜: è¾“å…¥æ•°æ®æœ¬èº«å˜åŒ–å¾ˆå° ({input_variation:.6f})")
        print(f"   å¯èƒ½åŸå› :")
        print(f"   1. æ•°æ®é›†è´¨é‡é—®é¢˜ - åœºæ™¯è¿‡äºç›¸ä¼¼")
        print(f"   2. æ•°æ®é¢„å¤„ç†è¿‡åº¦æ ‡å‡†åŒ–")
        print(f"   3. è§†é¢‘æ•°æ®æŸåæˆ–æ ¼å¼é—®é¢˜")
    else:
        print(f"âœ… è¾“å…¥è¾“å‡ºå˜åŒ–æ€§æ­£å¸¸")
        print(f"   è¾“å…¥å˜åŒ–æ€§: {input_variation:.6f}")
        print(f"   è¾“å‡ºå˜åŒ–æ€§: {output_variation:.6f}")

if __name__ == "__main__":
    analyze_multimodal_data()
